# Pytorch implementation of Attention is All You Need

Transformer implementation for Pretraining LLMs software project at UdS.

> [!Note]
> This repository is WIP. Stability will be achieved on v1.0.0.

## Usage

### Pretraining run

```bash
add pretraining code here
```

### Finetuning run

```bash
add finetuning code here
```

## Data downloading and preprocessing

```bash
add data downloading & preprocessing script 

```

## Pretraining Results

```bash
add loss curves, perplexity, etc

```

## Dashboard

```bash
add w&b link
```

## Checklist

- [ ] Model
- [ ] Dataloader
- [ ] Data processing
- [ ] Pretraining
- [ ] Evaluation

```bibtex
@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:13756489}
}
```
